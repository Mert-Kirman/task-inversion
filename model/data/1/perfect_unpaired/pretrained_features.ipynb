{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57402be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "images_np = np.load(\"../../perfect_unpaired/forward_context_perfect_unpaired_data.npy\")\n",
    "plt.imshow(images_np[0, 1:].reshape(224, 224), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6be4a502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Defining preprocessing pipeline...\n",
      "Preprocessing pipeline created successfully.\n",
      "------------------------------\n",
      "3. Loading pretrained ResNet50 model...\n",
      "ResNet50 model loaded and modified for feature extraction.\n",
      "------------------------------\n",
      "4. Setting up device and model mode...\n",
      "Using device: cpu\n",
      "------------------------------\n",
      "5. Extracting features...\n",
      "Processed 10/60 images...\n",
      "Processed 20/60 images...\n",
      "Processed 30/60 images...\n",
      "Processed 40/60 images...\n",
      "Processed 50/60 images...\n",
      "Processed 60/60 images...\n",
      "Processed 70/60 images...\n",
      "Processed 80/60 images...\n",
      "Processed 90/60 images...\n",
      "Processed 100/60 images...\n",
      "Processed 110/60 images...\n",
      "Processed 120/60 images...\n",
      "Processed 130/60 images...\n",
      "Processed 140/60 images...\n",
      "Processed 150/60 images...\n",
      "Processed 160/60 images...\n",
      "Processed 170/60 images...\n",
      "\n",
      "Feature extraction complete!\n",
      "------------------------------\n",
      "Shape of the final features tensor: torch.Size([170, 2048])\n",
      "First 5 features of the first image:\n",
      "tensor([0.0310, 0.0198, 0.0230,  ..., 0.0000, 0.0000, 0.0457])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Prepare your data\n",
    "# Let's create a dummy numpy array that matches your description.\n",
    "# In your actual code, you would load your own data here.\n",
    "\n",
    "images_forward_unpaired = np.load(\"../../perfect_unpaired/forward_context_perfect_unpaired_data.npy\")\n",
    "images_inverse_unpaired = np.load(\"../../perfect_unpaired/inverse_context_perfect_unpaired_data.npy\")\n",
    "images_extra = np.load(\"../context_extra_data.npy\")\n",
    "images_test = np.load(\"../context_test_data.npy\")\n",
    "images_np = np.concatenate((images_forward_unpaired, images_inverse_unpaired, images_extra, images_test), axis=0)\n",
    "images_np = images_np[:, 1:].reshape(-1, 224, 224)  # Assuming images are grayscale and reshaped to (N, 1, 224, 224)\n",
    "\n",
    "# Step 2: Define the preprocessing pipeline\n",
    "# ResNet50 was trained on 3-channel RGB images of size 224x224.\n",
    "# We need to transform our grayscale 200x200 images to match this.\n",
    "print(\"2. Defining preprocessing pipeline...\")\n",
    "\n",
    "# These are the standard ImageNet normalization values\n",
    "imagenet_mean = [0.485, 0.456, 0.406]\n",
    "imagenet_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "preprocess_pipeline = transforms.Compose([\n",
    "    # Convert a NumPy array to a PIL Image, as many transforms work with PIL\n",
    "    transforms.ToPILImage(),\n",
    "    \n",
    "    # Convert the PIL image to a tensor. This scales pixel values to [0.0, 1.0]\n",
    "    transforms.ToTensor(),\n",
    "    \n",
    "    # --- Crucial step for grayscale images ---\n",
    "    # ResNet50 expects 3 channels. We duplicate the grayscale channel 3 times.\n",
    "    # The tensor is now shape (1, 224, 224), we make it (3, 224, 224)\n",
    "    transforms.Lambda(lambda x: x.repeat(3, 1, 1)),\n",
    "    \n",
    "    # Normalize with ImageNet's mean and standard deviation\n",
    "    transforms.Normalize(mean=imagenet_mean, std=imagenet_std)\n",
    "])\n",
    "print(\"Preprocessing pipeline created successfully.\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# Step 3: Load the pretrained ResNet50 model\n",
    "print(\"3. Loading pretrained ResNet50 model...\")\n",
    "# Use the recommended modern way of loading weights\n",
    "weights = models.ResNet50_Weights.IMAGENET1K_V2\n",
    "model = models.resnet50(weights=weights)\n",
    "\n",
    "# To use the model for feature extraction, we remove the final fully connected layer (the classifier)\n",
    "# The output will be the 2048-dimensional feature vector from the 'avgpool' layer\n",
    "feature_extractor = nn.Sequential(*list(model.children())[:-1])\n",
    "print(\"ResNet50 model loaded and modified for feature extraction.\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# Step 4: Set up device (GPU or CPU) and put the model in evaluation mode\n",
    "print(\"4. Setting up device and model mode...\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "feature_extractor.to(device)\n",
    "# Set the model to evaluation mode. This is important as it disables layers like Dropout\n",
    "# and uses the learned running statistics for BatchNorm layers.\n",
    "feature_extractor.eval()\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# Step 5: Extract features from your images\n",
    "print(\"5. Extracting features...\")\n",
    "all_features = []\n",
    "num_images= 60\n",
    "# Disable gradient calculations for inference, which saves memory and computation\n",
    "with torch.no_grad():\n",
    "    for i, img_np in enumerate(images_np):\n",
    "        # Apply the preprocessing pipeline to the single numpy image\n",
    "        # This will return a tensor of shape (3, 224, 224)\n",
    "        input_tensor = preprocess_pipeline(img_np)\n",
    "        \n",
    "        # The model expects a batch of images, so we add a batch dimension\n",
    "        # Shape becomes (1, 3, 224, 224)\n",
    "        input_batch = input_tensor.unsqueeze(0)\n",
    "        \n",
    "        # Move the batch to the device\n",
    "        input_batch = input_batch.to(device)\n",
    "        \n",
    "        # Get the feature vector from the model\n",
    "        features = feature_extractor(input_batch)\n",
    "        \n",
    "        # The output of the feature extractor is of shape (1, 2048, 1, 1).\n",
    "        # We squeeze it to get a 1D vector of shape (2048).\n",
    "        features_squeezed = torch.squeeze(features)\n",
    "        \n",
    "        # Move features to CPU and convert to numpy if needed later, or just store the tensor\n",
    "        all_features.append(features_squeezed.cpu()) # .numpy() if you need numpy array\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Processed {i+1}/{num_images} images...\")\n",
    "\n",
    "# Stack all feature tensors into a single tensor\n",
    "# This will be a tensor of shape (60, 2048)\n",
    "final_features_tensor = torch.stack(all_features)\n",
    "\n",
    "features_np = final_features_tensor.numpy()  # Convert to numpy array if needed\n",
    "np.save(\"context_pretrained_features.npy\", features_np)\n",
    "\n",
    "print(\"\\nFeature extraction complete!\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Shape of the final features tensor: {final_features_tensor.shape}\")\n",
    "print(\"First 5 features of the first image:\")\n",
    "print(final_features_tensor[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20a7eed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original features shape: torch.Size([170, 2048])\n",
      "PCA complete!\n",
      "Shape of the new lower-dimensional training features: (60, 75)\n",
      "Components explain 99.95% of the variance in the training data.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "print(f\"Original features shape: {final_features_tensor.shape}\")\n",
    "\n",
    "# Convert tensor to numpy array for scikit-learn\n",
    "features_np = final_features_tensor.numpy()\n",
    "\n",
    "# Split the data into training (first 60) and test (remaining 30) sets\n",
    "train_features_np = features_np[60:140]\n",
    "\n",
    "# pca = PCA(n_components=target_dimension)\n",
    "pca = PCA(n_components=75)\n",
    "\n",
    "# Fit PCA on the training data only. [1, 2]\n",
    "pca.fit(train_features_np)\n",
    "\n",
    "# Apply the transformation to both training and test sets. [1]\n",
    "lower_dim_forward_unpaired = pca.transform(features_np[:60])\n",
    "lower_dim_inverse_unpaired = pca.transform(features_np[60:120])\n",
    "lower_dim_extra = pca.transform(features_np[120:140])\n",
    "lower_dim_test = pca.transform(features_np[140:])\n",
    "\n",
    "print(\"PCA complete!\")\n",
    "print(f\"Shape of the new lower-dimensional training features: {lower_dim_forward_unpaired.shape}\")\n",
    "\n",
    "# Check how much variance is explained by the components from the training data\n",
    "explained_variance = np.sum(pca.explained_variance_ratio_)\n",
    "print(f\"Components explain {explained_variance:.2%} of the variance in the training data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a00e2406",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"forward_context_pretrained_features_lower_train_dim75.npy\", np.concatenate((lower_dim_forward_unpaired, lower_dim_extra), axis=0))\n",
    "np.save(\"inverse_context_pretrained_features_lower_train_dim75.npy\", np.concatenate((lower_dim_inverse_unpaired, lower_dim_extra), axis=0))\n",
    "np.save(\"context_pretrained_features_lower_test_dim75.npy\", lower_dim_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "task_exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
